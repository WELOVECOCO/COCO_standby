{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f067ce0f",
   "metadata": {},
   "source": [
    "# BRIEF\n",
    "IT NOW CAN PERFORM REGRESSION AND multi class CLASSIFICATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66146b8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27935ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T12:49:54.019034Z",
     "start_time": "2025-02-05T12:49:45.980132Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape and add channel dimension\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))  # [B, H, W, C]\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))    # [B, H, W, C]\n",
    "\n",
    "# Convert to [B, C, H, W] format  # [B, C, H, W]\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_images = train_images.transpose(0, 3, 1, 2)\n",
    "test_images = test_images.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Train images shape:\", train_images.shape)  # Should be (60000, 1, 28, 28)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)    # Should be (10000, 1, 28, 28)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from core.Models import Model\n",
    "from core.nn import Linear, Conv2d, MaxPool2d , batchnorm2d, Softmax, Relu,Flatten\n",
    "from core.optim import sgd, adam\n",
    "from core.loss import get_loss_fn\n",
    "\n",
    "class ResNetWithResiduals(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial Convolution\n",
    "        self.conv1 = Conv2d(input_channels=1, output_channels=8, kernel_size=3, stride=1, padding=1, initialize_type='xavier',bias=False)\n",
    "        self.bn1 = batchnorm2d(8)\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        # Residual Block 1\n",
    "        self.conv2 = Conv2d(input_channels=8, output_channels=16, kernel_size=3, stride=1, padding=1, initialize_type='xavier')\n",
    "        self.bn2 = batchnorm2d(16)\n",
    "        self.relu2 = Relu()\n",
    "        self.res1 = Conv2d(input_channels=8, output_channels=16, kernel_size=1, stride=1, padding=0, initialize_type='xavier')  # Adjust channels\n",
    "        self.max1 = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Residual Block 2\n",
    "        self.conv3 = Conv2d(input_channels=16, output_channels=32, kernel_size=3, stride=1, padding=1, initialize_type='xavier')\n",
    "        self.bn3 = batchnorm2d(32)\n",
    "        self.relu3 = Relu()\n",
    "\n",
    "        self.conv4 = Conv2d(input_channels=32, output_channels=64, kernel_size=3, stride=1, padding=1, initialize_type='xavier')\n",
    "        self.bn4 = batchnorm2d(64)\n",
    "        self.relu4 = Relu()\n",
    "        self.res2 = Conv2d(input_channels=16, output_channels=64, kernel_size=1, stride=1, padding=0, initialize_type='xavier')  # Adjust channels\n",
    "        self.max2 = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(64 * 7 * 7, 100, initialize_type='xavier', activation='relu')\n",
    "        self.linear2 = Linear(100, 10, initialize_type='xavier', activation='softmax')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.relu1(x1)\n",
    "\n",
    "        # Residual Block 1\n",
    "        x2 = self.conv2(x1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.relu2(x2)\n",
    "        res1 = self.res1(x1)  # Match channel dimensions\n",
    "        x2 = x2 + res1  # Add residual connection\n",
    "        x2 = self.max1(x2)\n",
    "\n",
    "        # Residual Block 2\n",
    "        x3 = self.conv3(x2)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = self.relu3(x3)\n",
    "\n",
    "        x4 = self.conv4(x3)\n",
    "        x4 = self.bn4(x4)\n",
    "        x4 = self.relu4(x4)\n",
    "        res2 = self.res2(x2)  # Match channel dimensions\n",
    "        x4 = x4 + res2  # Add residual connection\n",
    "        x4 = self.max2(x4)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        x4 = self.flatten(x4)\n",
    "        x4 = self.linear1(x4)\n",
    "        x4 = self.linear2(x4)\n",
    "\n",
    "\n",
    "        return x4\n",
    "\n",
    "# Instantiate model\n",
    "model_resnet_residuals = ResNetWithResiduals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e915d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.Datasets import Dataset\n",
    "train_dataset = Dataset(train_images, train_labels, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039de797",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = adam(model_resnet_residuals.parameters(), learning_rate=0.001)\n",
    "loss_fn = get_loss_fn('categorical_bce')\n",
    "num_epochs = 1\n",
    "def train():\n",
    "    for epoch in range(num_epochs):\n",
    "        train_dataset.reset()  # Reset dataset iterator and reshuffle if needed\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for X_batch, y_batch in train_dataset:\n",
    "            optimizer.zero_grad()\n",
    "            out = model_resnet_residuals(X_batch)\n",
    "            loss_tensor = loss_fn.sparse_categorical_cross_entropy(y_batch, out)\n",
    "            epoch_loss += loss_tensor.data\n",
    "            num_batches += 1\n",
    "            loss_tensor.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f21795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_resnet_residuals.layers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce660f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdedf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model_resnet_residuals.test()\n",
    "    print(Config.TEST)\n",
    "    x = model_resnet_residuals(test_images)\n",
    "    y_pred = x.data\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1, keepdims=True)\n",
    "    y_true_classes = np.argmax(test_labels, axis=1, keepdims=True)  # Assuming test_labels are integers\n",
    "    accuracy = np.mean(y_pred_classes == y_true_classes)\n",
    "    print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_resnet_residuals.view_graph(input_data=test_images[:1],filename=\"model_graph\", format=\"png\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211253b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_residuals.save_model(filepath=\"model_resnet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf67e63",
   "metadata": {},
   "source": [
    "# Augmentation Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af844cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load CIFAR-10 (only training data for demonstration)\n",
    "(x_train, y_train), _ = cifar10.load_data()\n",
    "\n",
    "# Select a sample image (CIFAR-10 images are 32x32 RGB)\n",
    "sample_img = x_train[0]\n",
    "\n",
    "# Import Compose and all Preprocessing transforms from your package.\n",
    "from Preprocessing.transforms import (\n",
    "    Compose, GaussianBlur, GaussianNoise, Normalize, \n",
    "    RandomAffine, RandomCrop, RandomErasing, RandomHorizontalFlip, \n",
    "    RandomRotation, RandomVerticalFlip, Resize, ToTensor\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping transformation names to their instances.\n",
    "# For demonstration, we force application by setting probabilities to 1.0.\n",
    "aug_transforms = {\n",
    "    \"RandomHorizontalFlip\": RandomHorizontalFlip(p=1.0),\n",
    "    \"RandomVerticalFlip\":   RandomVerticalFlip(p=1.0),\n",
    "    \"RandomCrop\":           RandomCrop((28, 28)),  # Crop to 28x28 (from 32x32)\n",
    "    \"RandomRotation\":       RandomRotation(degrees=30),\n",
    "    \"RandomAffine\":         RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "    \"GaussianBlur\":         GaussianBlur(sigma=(0.5, 1.5)),\n",
    "    \"GaussianNoise\":        GaussianNoise(std=(0.01, 0.05)),\n",
    "    \"RandomErasing\":        RandomErasing(p=1.0, scale=(0.02, 0.15), ratio=(0.3, 3.3), value=0)\n",
    "}\n",
    "\n",
    "# Prepare the plot: one row per Preprocessing (each row shows Original and Augmented)\n",
    "n_transforms = len(aug_transforms)\n",
    "fig, axes = plt.subplots(nrows=n_transforms, ncols=2, figsize=(8, n_transforms * 3))\n",
    "\n",
    "for idx, (name, transform) in enumerate(aug_transforms.items()):\n",
    "    # Create a Compose pipeline with a single transform.\n",
    "    pipeline = Compose([transform])\n",
    "    \n",
    "    # Left column: display the original image.\n",
    "    axes[idx, 0].imshow(sample_img)\n",
    "    axes[idx, 0].set_title(\"Original\")\n",
    "    axes[idx, 0].axis(\"off\")\n",
    "    \n",
    "    # Right column: apply the Compose pipeline and display the result.\n",
    "    aug_img = pipeline(sample_img)\n",
    "    if hasattr(aug_img, \"data\"):\n",
    "        aug_img = aug_img.data\n",
    "    axes[idx, 1].imshow(np.clip(aug_img, 0, 255).astype(np.uint8))\n",
    "    axes[idx, 1].set_title(name)\n",
    "    axes[idx, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05fcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502faa75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe4b0941",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Download the pre-trained ResNet-18 model\n",
    "pytorch_resnet18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1, progress=True)\n",
    "state_dict = pytorch_resnet18.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,_ in state_dict.items():\n",
    "    print(key,\"size:\",state_dict[key].size())\n",
    "\n",
    "#remove num_batches_tracked from state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state_dict (assuming you already have it)\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if \"num_batches_tracked\" not in k}\n",
    "print(len(filtered_state_dict))\n",
    "# Print the new state_dict keys to verify\n",
    "for key in filtered_state_dict.keys():\n",
    "    print(\"size:\", filtered_state_dict[key].size())\n",
    "\n",
    "# Save or use the filtered state_dict as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "\n",
    "from pretrained.resnet18 import resnet18\n",
    "from core.Models import Model\n",
    "model = resnet18(pretrained=True)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from core.tensor import Tensor\n",
    "label = 153\n",
    "imagepath = r\"C:\\Users\\ahmed\\Downloads\\n02085936_Maltese_dog (1).JPEG\"\n",
    "\n",
    "img = Image.open(imagepath)\n",
    "img = img.resize((224, 224))\n",
    "img = np.array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = np.transpose(img, (0, 3, 1, 2))  # convert to (batch_size, channels, height, width)\n",
    "img = img / 255.0\n",
    "img = Tensor(img)\n",
    "\n",
    "output = model(img)\n",
    "y_pred = output.data\n",
    "print(y_pred.shape)\n",
    "#print top 5 predictions\n",
    "print(\"Top 5 predictions:\", np.argsort(y_pred[0])[::-1][:5])\n",
    "# pred_label = np.argmax(y_pred[0], axis=0, keepdims=False)\n",
    "# print(\"Predicted label:\", pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrained.vgg16 import VGG16\n",
    "vgg = VGG16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from core.tensor import Tensor\n",
    "label = 153\n",
    "imagepath = r\"C:\\Users\\ahmed\\Downloads\\n02085936_Maltese_dog (1).JPEG\"\n",
    "\n",
    "img = Image.open(imagepath)\n",
    "img = img.resize((224, 224))\n",
    "img = np.array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = np.transpose(img, (0, 3, 1, 2))  # convert to (batch_size, channels, height, width)\n",
    "img = img / 255.0\n",
    "img = Tensor(img)\n",
    "\n",
    "output = vgg(img)\n",
    "y_pred = output.data\n",
    "pred_label = np.argmax(y_pred[0], axis=0, keepdims=False)\n",
    "print(\"Top 5 predictions:\", np.argsort(y_pred[0])[::-1][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb47e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.Models import Model\n",
    "from core.nn import PositionalEmbedding , PatchEmbedding , LayerNorm\n",
    "\n",
    "class model(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "        self.positional_embedding = PositionalEmbedding(self.patch_embedding.n_patches, 768)\n",
    "        self.layer_norm = LayerNorm(768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5080cb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'std'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model()\n\u001b[1;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m      6\u001b[0m output\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\COCO_standby\\core\\Models.py:72\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, x, test)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    ARGS: \u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m        x (ndarray): Input data (e.g., image: [B, C, H, W] or feature vector: [B, D])\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    RETURNS:\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        ndarray: Output probabilities (B, num_output neurons)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_out\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mmodel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embedding(x)\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding(x)\n\u001b[1;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\COCO_standby\\core\\nn.py:664\u001b[0m, in \u001b[0;36mLayerNorm.__call__\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    663\u001b[0m     mean \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 664\u001b[0m     std \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    665\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m (std \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m x_norm \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'std'"
     ]
    }
   ],
   "source": [
    "from core.tensor import Tensor\n",
    "import numpy as np\n",
    "x = Tensor(np.random.rand(1, 3, 224, 224))\n",
    "model = model()\n",
    "output = model(x)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from core.tensor import Tensor\n",
    "from core.nn import MultiHeadAttention\n",
    "attention_layer = MultiHeadAttention(dmodel=8, nheads=2, masked=True)\n",
    "\n",
    "# Generate a random input tensor\n",
    "B, T, dmodel = 2, 7, 8  # Batch size = 4, Sequence length = 5, Embedding size = 8\n",
    "x = np.random.rand(B, T, dmodel)  # Random input\n",
    "x = Tensor(x)\n",
    "# Forward pass\n",
    "output = attention_layer(x)\n",
    "\n",
    "# Print results\n",
    "print(\"Input shape: \", x.shape)        # Expected: (4, 5, 8)\n",
    "print(\"Output shape:\", output.shape)   # Expected: (4, 5, 8)\n",
    "# print(\"output type:\", output)\n",
    "\n",
    "# print(\"Output values:\\n\", output.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fe8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = output.backward()\n",
    "print(output.grad.shape)  # Expected: (4, 5, 8)\n",
    "print(\"outputGrad values:\\n\", output.grad)\n",
    "print(x.grad.shape)  # Expected: (4, 5, 8)\n",
    "print(\"xGrad values:\\n\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0454243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

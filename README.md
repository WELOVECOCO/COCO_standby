# Mini Deep Learning Framework for training feed forward neural networks

This mini deep learning framework is designed for training feed-forward neural networks. It is implemented entirely from scratch, providing a flexible and straightforward way to build and train neural networks.

## Purpose

The primary goal of this framework is educational. It aims to help users understand the inner workings of neural networks by providing a hands-on experience with the fundamental components involved in building and training these models. By using this framework, you can gain insights into what happens behind the scenes during the training process of neural networks.

## Features

- **Layer Initialization**: Supports multiple weight initialization methods including He, Xavier, and Lecun.
- **Activation Functions**: Includes Sigmoid, Softmax, ReLU, Leaky ReLU, Tanh, and no activation.
- **Optimizers**: Implements several optimization algorithms like SGD, Momentum, NAG, Adam, RMSprop, and NAdam.
- **Dropout Regularization**: Provides dropout regularization for each layer.
- **Batch Normalization**: Normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.
- **Forward and Backward Propagation**: Supports standard forward and backward propagation for training neural networks.
- **Gradient Calculation**: Includes L1 and L2 regularization.


## Contributing

If you want to contribute to this project, please fork the repository and submit a pull request.